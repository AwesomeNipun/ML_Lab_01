# -*- coding: utf-8 -*-
"""190653L_ML_Lab_1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K4XRJYcuBH1H0-Dls49cjHQnDt0spEv9
"""

import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sn
import math
from sklearn.metrics import mean_squared_error

# Specify the file paths
train_path = "/content/drive/MyDrive/ML Lab 1/train.csv"
valid_path = "/content/drive/MyDrive/ML Lab 1/valid.csv"
test_path = "/content/drive/MyDrive/ML Lab 1/test.csv"

# Load the CSV files into Pandas DataFrames
train = pd.read_csv(train_path)
valid = pd.read_csv(valid_path)
test = pd.read_csv(test_path)

train_ = train.copy()
valid_ = valid.copy()
test_ = test.copy()

"""# KNN Classifier"""

knn = KNeighborsClassifier(n_neighbors=1)
def knn_classifier(x_train, y_train, x_val, y_val):

  knn.fit(np.array(x_train), y_train)

  y_pred = knn.predict(np.array(x_val))

  accuracy = accuracy_score(y_val, y_pred)
  print(f"Accuracy: {accuracy *100:.2f} %")
  return accuracy

"""# Label 1"""

# Remove rows where label 1 is NaN in the 'train' DataFrame
train_cleaned = train_[~np.isnan(train_['label_1'])]

# Remove rows where label 1 is NaN in the 'valid' DataFrame
valid_cleaned = valid_[~np.isnan(valid_['label_1'])]

train_label = train_cleaned[['label_1', 'label_2', 'label_3', 'label_4']]
valid_label = valid_cleaned[['label_1', 'label_2', 'label_3', 'label_4']]
# test_label = test_[['label_1', 'label_2', 'label_3', 'label_4']]

train_features = train_cleaned.iloc[:, :-4]
valid_features = valid_cleaned.iloc[:, :-4]
test_features = test.iloc[:, :-4]

train_features.head()

train_features_with_label = train_cleaned.iloc[:, :-3]
valid_features_with_label = valid_cleaned.iloc[:, :-3]

train_features_with_label.head()

plt.figure(figsize=(18, 6))
sn.countplot(data=train_label, x='label_1', color='purple')

knn_classifier(train_features, train_label['label_1'], valid_features, valid_label['label_1'])

test_features.head()

label1_pred_test_before_fe = knn.predict(np.array(test_features))

"""# Correlation"""

train_corr_matrix = train_features.corr()

def get_correlated_features(corr_matrix):
  threshold = 0.5
  correlated_features = set()
  for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > threshold:
            colname = corr_matrix.columns[i]
            correlated_features.add(colname)

  return correlated_features

train_corr_features = get_correlated_features(train_corr_matrix)

# Remove the specified features from the 'train_cleaned' DataFrame
train_features_with_label_1 = train_features_with_label.drop(columns=list(train_corr_features))
# Remove the specified features from the 'valid_cleaned' DataFrame
valid_features_with_label_1 = valid_features_with_label.drop(columns=list(train_corr_features))
# Remove the specified features from the 'test_cleaned' DataFrame
test_features = test_features.drop(columns=list(train_corr_features))

train_features_with_label_1.head()

print(len(train_corr_features))

train_corr_matrix_with_label_1 = train_features_with_label_1.corr()

# train_corr_matrix_with_label.head()

def get_correlated_features_by_label(corr_matrix, label_name):

    threshold = 0.01
    # Extract the column index of the specified label column
    label_col_index = corr_matrix.columns.get_loc(label_name)

    # Get the correlation values between features and the specified label
    correlations_with_label = corr_matrix.iloc[:, label_col_index]

    # Filter for features that are correlated with the label above the threshold
    correlated_features = correlations_with_label[correlations_with_label.index != label_name]
    correlated_features = correlated_features[correlated_features.abs() < threshold]

    return correlated_features.index.tolist()

new_train_corr_features = get_correlated_features_by_label(train_corr_matrix_with_label_1, 'label_1')

# Remove the specified features
train_after_corr = train_features_with_label_1.drop(columns=new_train_corr_features)

# Remove the specified features
valid_after_corr = valid_features_with_label_1.drop(columns=new_train_corr_features)

# Remove the specified features
test_after_corr = test_features.drop(columns=new_train_corr_features)

print(len(new_train_corr_features))

train_fe = train_after_corr.drop(columns=['label_1'])
valid_fe = valid_after_corr.drop(columns=['label_1'])

knn_classifier(train_fe, train_label['label_1'], valid_fe, valid_label['label_1'] )

"""## Standardization"""

from sklearn.preprocessing import RobustScaler

train_after_corr.head()

valid_after_corr.head()

test_after_corr.head()

# remove lable for PCA
train = train_after_corr.drop(columns=["label_1"])
valid = valid_after_corr.drop(columns=["label_1"])
test = test_after_corr

scaler = RobustScaler()

# Fit and transform the training data
train_standardized = scaler.fit_transform(train)

# Transform the validation data using the same scaler
valid_standardized = scaler.transform(valid)

# Transform the test data using the same scaler
test_standardized = scaler.transform(test)

"""## PCA"""

from sklearn.decomposition import PCA

threshold = 0.95

pca = PCA(threshold, svd_solver='full')

pca_train = pca.fit_transform(train_standardized)
pca_valid = pca.transform(valid_standardized)
pca_test = pca.transform(test_standardized)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for train data:", retained_features_len)

min_components = 256
threshold = 0.95
while True:
    pca = PCA(n_components=threshold, svd_solver='full')

    min_components = pca_train.shape[1]
    pca_train = pca.fit_transform(pca_train)
    pca_valid = pca.transform(pca_valid)
    pca_test = pca.transform(pca_test)

    accuracy_val = knn_classifier(pca_train, train_label['label_1'], pca_valid, valid_label['label_1'])
    if accuracy_val < 0.975:
        break

print(min_components)

pca = PCA(n_components=min_components, svd_solver = 'full')

pca_train = pca.fit_transform(train_standardized)
pca_valid = pca.transform(valid_standardized)
pca_test = pca.transform(test_standardized)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for train data:", retained_features_len)

knn_classifier(pca_train, train_label['label_1'], pca_valid, valid_label['label_1'])

label1_pred_test_after_fe = knn.predict(np.array(pca_test))

label_1_features = pd.DataFrame(data=pca_test, columns=[f'new_feature_{i+1}' for i in range(pca_test.shape[1])])
label_1_features.insert(0,'Predicted labels before feature engineering',label1_pred_test_before_fe)
label_1_features.insert(1,'Predicted labels after feature engineering', label1_pred_test_after_fe)
label_1_features.insert(2,'No of new features', pca_test.shape[1])

label_1_features.head()

plt.bar(range(1, len(explained_variance)+1), explained_variance)

plt.plot(range(1, len(explained_variance)+1), np.cumsum(explained_variance),
         c='red', label='Label_1')

plt.legend(loc='upper left')
plt.xlabel('Number of componenets')
plt.ylabel('Explained Variance')
plt.title('Plot')

plt.show()

"""## Label 2

"""

# Remove rows where label 1 is NaN in the 'train' DataFrame
train_cleaned_label_2 = train_[~np.isnan(train_['label_2'])]

# Remove rows where label 1 is NaN in the 'valid' DataFrame
valid_cleaned_label_2 = valid_[~np.isnan(valid_['label_2'])]

train_label_2 = train_cleaned_label_2[['label_1', 'label_2', 'label_3', 'label_4']]
valid_label_2 = valid_cleaned_label_2[['label_1', 'label_2', 'label_3', 'label_4']]
# test_label_2 = test_[['label_1', 'label_2', 'label_3', 'label_4']]

train_label_2['label_2'].head()

train_features_label_2 = train_cleaned_label_2.iloc[:, :-4]
valid_features_label_2 = valid_cleaned_label_2.iloc[:, :-4]
test_features_label_2 = test_.iloc[:, :-4]

train_features_label_2.head()

train_features_with_label_2 = train_cleaned_label_2.iloc[:, :-2].drop(columns=['label_1'])
valid_features_with_label_2 = valid_cleaned_label_2.iloc[:, :-2].drop(columns=['label_1'])

train_features_with_label_2.head()

plt.figure(figsize=(18, 6))
sn.histplot(data=train_label_2, x='label_2', bins=20, kde=False)

import xgboost as xgb

model = xgb.XGBRegressor()

model.fit(train_features_label_2, train_label_2['label_2'])
y_pred = model.predict(valid_features_label_2)

score = math.sqrt(mean_squared_error(valid_label_2['label_2'], y_pred))

print('Test Score: %.2f RMSE' % (score))

label2_pred_test_before_fe = model.predict(test_features_label_2)

train_corr_matrix_2 = train_features_label_2.corr()

train_corr_features_2 = get_correlated_features(train_corr_matrix_2)


train_features_with_label_2 = train_features_with_label_2.drop(columns=list(train_corr_features_2))
valid_features_with_label_2 = valid_features_with_label_2.drop(columns=list(train_corr_features_2))
test_features_label_2 = test_features_label_2.drop(columns=list(train_corr_features_2))

train_features_with_label_2.head()

print(len(train_corr_features_2))

train_corr_matrix_with_label_2 = train_features_with_label_2.corr()

new_train_corr_features_2 = get_correlated_features_by_label(train_corr_matrix_with_label_2, 'label_2')

# Remove the specified features
train_after_corr_2 = train_features_with_label_2.drop(columns=new_train_corr_features_2)

# Remove the specified features
valid_after_corr_2 = valid_features_with_label_2.drop(columns=new_train_corr_features_2)

# Remove the specified features
test_after_corr_2 = test_features_label_2.drop(columns=new_train_corr_features_2)

print(len(new_train_corr_features_2))

# remove lable for PCA
train_2 = train_after_corr_2.drop(columns=["label_2"])
valid_2 = valid_after_corr_2.drop(columns=["label_2"])
test_2 = test_after_corr_2

train_2.head()

scaler = RobustScaler()

train_standardized_2 = scaler.fit_transform(train_2)
valid_standardized_2 = scaler.transform(valid_2)
test_standardized_2 = scaler.transform(test_2)

pca = PCA(threshold, svd_solver='full')

pca_train_2 = pca.fit_transform(train_standardized_2)
pca_valid_2 = pca.transform(valid_standardized_2)
pca_test_2 = pca.transform(test_standardized_2)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for train data:", retained_features_len)

min_components = 256
threshold = 0.95
while True:
    pca = PCA(n_components=threshold, svd_solver='full')

    min_components = pca_train_2.shape[1]
    pca_train_2 = pca.fit_transform(pca_train_2)
    pca_valid_2 = pca.transform(pca_valid_2)
    pca_test_2 = pca.transform(pca_test_2)

    model = xgb.XGBRegressor()
    model.fit(pca_train_2, train_label_2['label_2'])
    y_pred = model.predict(pca_valid_2)

    score_val = math.sqrt(mean_squared_error(valid_label_2['label_2'], y_pred))
    if score_val > 3.8:
        break

print(min_components)

pca = PCA(n_components=min_components, svd_solver = 'full')

pca_train_2 = pca.fit_transform(train_standardized_2)
pca_valid_2 = pca.transform(valid_standardized_2)
pca_test_2 = pca.transform(test_standardized_2)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for train data:", retained_features_len)

model = xgb.XGBRegressor()
model.fit(pca_train_2, train_label_2['label_2'])
y_pred = model.predict(pca_valid_2)

score_val = math.sqrt(mean_squared_error(valid_label_2['label_2'], y_pred))
print('Test Score: %.2f RMSE' % (score_val))

label2_pred_test_after_fe = model.predict(pca_test_2)

label_2_features = pd.DataFrame(data=pca_test_2, columns=[f'new_feature_{i+1}' for i in range(pca_test_2.shape[1])])
label_2_features.insert(0,'Predicted labels before feature engineering',label2_pred_test_before_fe)
label_2_features.insert(1,'Predicted labels after feature engineering', label2_pred_test_after_fe)
label_2_features.insert(2,'No of new features', pca_test_2.shape[1])

label_2_features.head()

plt.bar(range(1, len(explained_variance)+1), explained_variance)

plt.plot(range(1, len(explained_variance)+1), np.cumsum(explained_variance),
         c='red', label='Label_2')

plt.legend(loc='upper left')
plt.xlabel('Number of componenets')
plt.ylabel('Explained Variance')
plt.title('Plot')

plt.show()

"""# Label 3"""

# Remove rows where label 3 is NaN in the 'train' DataFrame
train_cleaned_label_3 = train_[~np.isnan(train_['label_3'])]

# Remove rows where label 3 is NaN in the 'valid' DataFrame
valid_cleaned_label_3 = valid_[~np.isnan(valid_['label_3'])]

train_label_3 = train_cleaned_label_3[['label_1', 'label_2', 'label_3', 'label_4']]
valid_label_3 = valid_cleaned_label_3[['label_1', 'label_2', 'label_3', 'label_4']]
# test_label_2 = test_[['label_1', 'label_2', 'label_3', 'label_4']]

train_label_3['label_3'].head()

train_features_label_3 = train_cleaned_label_3.iloc[:, :-4]
valid_features_label_3 = valid_cleaned_label_3.iloc[:, :-4]
test_features_label_3= test_.iloc[:, :-4]

train_features_label_3.head()

train_features_with_label_3 = train_cleaned_label_3.iloc[:, :-1].drop(columns=['label_1', 'label_2'])
valid_features_with_label_3 = valid_cleaned_label_3.iloc[:, :-1].drop(columns=['label_1', 'label_2'])

train_features_with_label_3.head()

x = sn.countplot(x=train_label_3['label_3'])

for p in x.patches:
    x.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=9, color='black')

from imblearn.over_sampling import RandomOverSampler

sampler = RandomOverSampler(random_state=0, sampling_strategy=0.80)
train_features_resampled, train_label_resampled = sampler.fit_resample(train_features_label_3, train_label_3['label_3'])

x = sn.countplot(x=train_label_resampled)

for p in x.patches:
    x.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=9, color='black')

knn_classifier(train_features_resampled, train_label_resampled, valid_features_label_3, valid_label_3['label_3'])

label3_pred_test_before_fe = knn.predict(np.array(test_features_label_3))

train_corr_matrix_3 = train_features_resampled.corr()
train_features_resampled.head()

train_features_with_label_3 = pd.concat([train_features_resampled, train_label_resampled], axis=1)
train_features_with_label_3.head()

train_corr_features_3= get_correlated_features(train_corr_matrix_3)

train_features_with_label_3 = train_features_with_label_3.drop(columns=list(train_corr_features_3))
valid_features_with_label_3 = valid_features_with_label_3.drop(columns=list(train_corr_features_3))
test_features_label_3 = test_features_label_3.drop(columns=list(train_corr_features_3))

train_features_with_label_3.head()

print(len(train_corr_features_3))

train_corr_matrix_with_label_3 = train_features_with_label_3.corr()

new_train_corr_features_3 = get_correlated_features_by_label(train_corr_matrix_with_label_3, 'label_3')

train_after_corr_3 = train_features_with_label_3.drop(columns=new_train_corr_features_3)
valid_after_corr_3 = valid_features_with_label_3.drop(columns=new_train_corr_features_3)
test_after_corr_3 = test_features_label_3.drop(columns=new_train_corr_features_3)

print(len(new_train_corr_features_3))

train_3 = train_after_corr_3.drop(columns=["label_3"])
valid_3 = valid_after_corr_3.drop(columns=["label_3"])
test_3 = test_after_corr_3

train_label_3 = train_features_with_label_3

knn_classifier(train_3, train_label_3['label_3'], valid_3, valid_label_3['label_3'])

scaler = RobustScaler()

train_standardized_3 = scaler.fit_transform(train_3)
valid_standardized_3 = scaler.transform(valid_3)
test_standardized_3 = scaler.transform(test_3)

pca = PCA(threshold, svd_solver='full')

pca_train_3 = pca.fit_transform(train_standardized_3)
pca_valid_3 = pca.transform(valid_standardized_3)
pca_test_3 = pca.transform(test_standardized_3)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for train data:", retained_features_len)

min_components = [256]
threshold = 0.95
accuracy_val = 1

while True:

    pca = PCA(n_components=threshold, svd_solver='full')


    pca_train_3 = pca.fit_transform(pca_train_3)
    pca_valid_3 = pca.transform(pca_valid_3)
    pca_test_3 = pca.transform(pca_test_3)

    accuracy_val = knn_classifier(pca_train_3, train_after_corr_3['label_3'], pca_valid_3, valid_label_3['label_3'])
    if accuracy_val < 1:
      break
    min_components.append(pca_train_3.shape[1])


print(min_components[-3])

pca = PCA(n_components=min_components[-3], svd_solver = 'full')

pca_train_3 = pca.fit_transform(train_standardized_3)
pca_valid_3 = pca.transform(valid_standardized_3)
pca_test_3 = pca.transform(test_standardized_3)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for train data:", retained_features_len)

knn_classifier(pca_train_3, train_after_corr_3['label_3'], pca_valid_3, valid_label_3['label_3'])

label3_pred_test_after_fe = knn.predict(np.array(pca_test_3))

label_3_features = pd.DataFrame(data=pca_test_3, columns=[f'new_feature_{i+1}' for i in range(pca_test_3.shape[1])])
label_3_features.insert(0,'Predicted labels before feature engineering',label3_pred_test_before_fe)
label_3_features.insert(1,'Predicted labels after feature engineering', label3_pred_test_after_fe)
label_3_features.insert(2,'No of new features', pca_test_3.shape[1])

label_3_features.head()

plt.bar(range(1, len(explained_variance)+1), explained_variance)

plt.plot(range(1, len(explained_variance)+1), np.cumsum(explained_variance),
         c='red', label='Label_3')

plt.legend(loc='upper left')
plt.xlabel('Number of componenets')
plt.ylabel('Explained Variance')
plt.title('Plot')

plt.show()

"""# Label 4"""

train_cleaned_label_4 = train_[~np.isnan(train_['label_4'])]
valid_cleaned_label_4 = valid_[~np.isnan(valid_['label_4'])]

train_label_4 = train_cleaned_label_4[['label_1', 'label_2', 'label_3', 'label_4']]
valid_label_4 = valid_cleaned_label_4[['label_1', 'label_2', 'label_3', 'label_4']]
# test_label_2 = test_[['label_1', 'label_2', 'label_3', 'label_4']]

train_label_4['label_4'].head()

train_features_label_4 = train_cleaned_label_4.iloc[:, :-4]
valid_features_label_4 = valid_cleaned_label_4.iloc[:, :-4]
test_features_label_4= test_.iloc[:, :-4]

train_features_label_4.head()

train_features_with_label_4 = train_cleaned_label_4.iloc[:, :].drop(columns=['label_1', 'label_2', 'label_3'])
valid_features_with_label_4 = valid_cleaned_label_4.iloc[:, :].drop(columns=['label_1', 'label_2', 'label_3'])

train_features_with_label_4.head()

plt.figure(figsize=(18, 6))
x = sn.countplot(x=train_label_4['label_4'], color='purple')

for p in x.patches:
    x.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=9, color='black')

sampler = RandomOverSampler(random_state=0)
train_features_resampled, train_label_resampled = sampler.fit_resample(train_features_label_4, train_label_4['label_4'])

plt.figure(figsize=(18, 6))
x= sn.countplot(x=train_label_resampled, color='purple')

for p in x.patches:
    x.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=9, color='black')

knn_classifier(train_features_resampled, train_label_resampled, valid_features_label_4, valid_label_4['label_4'])

label4_pred_test_before_fe = knn.predict(np.array(test_features_label_4))

train_corr_matrix_4 = train_features_resampled.corr()
train_features_resampled.head()

train_features_with_label_4 = pd.concat([train_features_resampled, train_label_resampled], axis=1)
train_features_with_label_4.head()

train_corr_features_4= get_correlated_features(train_corr_matrix_4)

train_features_with_label_4 = train_features_with_label_4.drop(columns=list(train_corr_features_4))
valid_features_with_label_4 = valid_features_with_label_4.drop(columns=list(train_corr_features_4))
test_features_label_4 = test_features_label_4.drop(columns=list(train_corr_features_4))

train_features_with_label_4.head()

print(len(train_corr_features_3))

train_corr_matrix_with_label_4 = train_features_with_label_4.corr()

new_train_corr_features_4 = get_correlated_features_by_label(train_corr_matrix_with_label_4, 'label_4')

train_after_corr_4 = train_features_with_label_4.drop(columns=new_train_corr_features_4)
valid_after_corr_4 = valid_features_with_label_4.drop(columns=new_train_corr_features_4)
test_after_corr_4 = test_features_label_4.drop(columns=new_train_corr_features_4)

print(len(new_train_corr_features_4))

train_4 = train_after_corr_4.drop(columns=["label_4"])
valid_4 = valid_after_corr_4.drop(columns=["label_4"])
test_4 = test_after_corr_4

train_label_4 = train_features_with_label_4

knn_classifier(train_4, train_label_4['label_4'], valid_4, valid_label_4['label_4'])

scaler = RobustScaler()

train_standardized_4 = scaler.fit_transform(train_4)
valid_standardized_4 = scaler.transform(valid_4)
test_standardized_4 = scaler.transform(test_4)

pca = PCA(threshold, svd_solver='full')

pca_train_4 = pca.fit_transform(train_standardized_4)
pca_valid_4 = pca.transform(valid_standardized_4)
pca_test_4 = pca.transform(test_standardized_4)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for train data:", retained_features_len)

min_components = 256
threshold = 0.95
while True:
    pca = PCA(n_components=threshold, svd_solver='full')

    min_components = pca_train_4.shape[1]
    pca_train_4 = pca.fit_transform(pca_train_4)
    pca_valid_4 = pca.transform(pca_valid_4)
    pca_test_4 = pca.transform(pca_test_4)

    accuracy_val = knn_classifier(pca_train_4, train_after_corr_4['label_4'], pca_valid_4, valid_label_4['label_4'])
    if accuracy_val < 0.985:
        break

print(min_components)

pca = PCA(n_components=min_components, svd_solver = 'full')

pca_train_4 = pca.fit_transform(train_standardized_4)
pca_valid_4 = pca.transform(valid_standardized_4)
pca_test_4 = pca.transform(test_standardized_4)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for train data:", retained_features_len)

knn_classifier(pca_train_4, train_after_corr_4['label_4'], pca_valid_4, valid_label_4['label_4'])

label4_pred_test_after_fe = knn.predict(np.array(pca_test_4))

label_4_features = pd.DataFrame(data=pca_test_4, columns=[f'new_feature_{i+1}' for i in range(pca_test_4.shape[1])])
label_4_features.insert(0,'Predicted labels before feature engineering',label4_pred_test_before_fe)
label_4_features.insert(1,'Predicted labels after feature engineering', label4_pred_test_after_fe)
label_4_features.insert(2,'No of new features', pca_test_4.shape[1])

label_4_features.head()

plt.bar(range(1, len(explained_variance)+1), explained_variance)

plt.plot(range(1, len(explained_variance)+1), np.cumsum(explained_variance),
         c='red', label='Label_4')

plt.legend(loc='upper left')
plt.xlabel('Number of componenets')
plt.ylabel('Explained Variance')
plt.title('Plot')

plt.show()

"""# Saving output to CSV files"""

import os

import warnings
warnings.filterwarnings('ignore')


if not os.path.exists('output'):
    os.makedirs('output')

def write_csv(feature, label):
  for i in range(feature['No of new features'][0], 256):
        feature[f'new_feature_{i+1}'] = pd.NA
  filename = f'output/190653L_label_{label}.csv'
  feature.to_csv(filename, index=False)

write_csv(label_1_features.copy(), 1)
write_csv(label_2_features.copy(), 2)
write_csv(label_3_features.copy(), 3)
write_csv(label_4_features.copy(), 4)

"""# Final Output"""

print (f"Features Count after apply feature engineering for Speaker ID: {label_1_features['No of new features'][0]}")
print (f"Features Count after apply feature engineering for Speaker Age: {label_2_features['No of new features'][0]}")
print (f"Features Count after apply feature engineering for Speaker Gender: {label_3_features['No of new features'][0]}")
print (f"Features Count after apply feature engineering for Speaker Accent: {label_4_features['No of new features'][0]}")